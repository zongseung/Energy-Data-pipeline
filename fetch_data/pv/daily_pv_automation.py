import asyncio
import aiohttp
import json
import pandas as pd
import xml.etree.ElementTree as ET
import os
import re
from pathlib import Path
from datetime import datetime, timedelta, date as date_
from dotenv import load_dotenv
from sqlalchemy import create_engine, text

load_dotenv()

# 1. í™˜ê²½ ë° DB ì„¤ì •
current_file = Path(__file__).resolve()
PROJECT_ROOT = current_file.parent.parent.parent
load_dotenv(PROJECT_ROOT / ".env")

# plant.jsonì—ì„œ gencd -> plant_name ë§¤í•‘ ë¡œë“œ (íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ dict)
_PLANT_JSON = PROJECT_ROOT / "plant.json"
GENCD_TO_NAME: dict[str, str] = {}
if _PLANT_JSON.exists():
    with open(_PLANT_JSON, "r", encoding="utf-8") as _f:
        for _p in json.load(_f):
            GENCD_TO_NAME.setdefault(_p["plant_code"], _p["plant_name"])

ENDPOINT = "https://apis.data.go.kr/B552520/PwrSunLightInfo/getDataService"

# lazy ì´ˆê¸°í™”: import ì‹œì ì´ ì•„ë‹Œ ì‹¤í–‰ ì‹œì ì— ê²€ì¦
_engine = None


def _get_api_key() -> str:
    key = os.getenv("NAMBU_API_KEY")
    if not key:
        raise RuntimeError("NAMBU_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    return key


def _get_engine():
    global _engine
    if _engine is None:
        db_url = os.getenv("DB_URL") or os.getenv("PV_DATABASE_URL") or os.getenv("LOCAL_DB_URL")
        if not db_url:
            raise RuntimeError("DB_URL(ë˜ëŠ” PV_DATABASE_URL/LOCAL_DB_URL)ì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        _engine = create_engine(db_url)
    return _engine

def _count_hours_for_day(engine_, gencd: str, hogi: int, day: date_) -> int:
    q = text(
        """
        SELECT COUNT(DISTINCT EXTRACT(HOUR FROM datetime)) AS hours
        FROM nambu_generation
        WHERE gencd = :gencd
          AND hogi = :hogi
          AND datetime >= :day_start
          AND datetime < :day_end
        """
    )
    day_start = datetime.combine(day, datetime.min.time())
    day_end = day_start + timedelta(days=1)
    with engine_.connect() as conn:
        return int(
            conn.execute(
                q,
                {"gencd": gencd, "hogi": hogi, "day_start": day_start, "day_end": day_end},
            ).scalar()
            or 0
        )


def get_active_targets(engine_):
    """
    nambu_generation í…Œì´ë¸”ì„ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì§‘ ëŒ€ìƒì„ ì •í•˜ê³ ,
    ë§ˆì§€ë§‰ ê¸°ë¡(ë˜ëŠ” ë¯¸ì™„ì„± ì¼ì)ì„ í™•ì¸í•˜ì—¬ ë°±í•„ ì‹œì‘ì¼ì„ ê²°ì •í•©ë‹ˆë‹¤.
    """
    query = """
    SELECT
        gencd,
        hogi,
        MAX(datetime) AS last_dt,
        MAX(plant_name) AS plant_name
    FROM nambu_generation
    GROUP BY gencd, hogi
    """
    with engine_.connect() as conn:
        df = pd.read_sql(text(query), conn)
    
    active_targets = []
    yesterday = datetime.now() - timedelta(days=1)

    for _, row in df.iterrows():
        gencd = str(row["gencd"]).strip()
        hogi = int(row["hogi"])
        plant_name = row.get("plant_name")
        last_dt = row["last_dt"]
        
        # í•„í„°ë§: ë§ˆì§€ë§‰ ê¸°ë¡ì´ 2025ë…„ ì´ì „ì´ë©´ ë°œì „ ì¤‘ë‹¨ìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ìŠ¤í‚µ
        if last_dt and last_dt.year < 2025:
            print(f"â© {gencd} ({hogi}í˜¸ê¸°): {last_dt.year}ë…„ ì´í›„ ê¸°ë¡ ì—†ìŒ. ìˆ˜ì§‘ ì œì™¸.")
            continue
            
        # ì‹œì‘ ë‚ ì§œ ê²°ì •:
        # - ë§ˆì§€ë§‰ ë‚ ì§œì˜ ì‹œê°„ ë°ì´í„°ê°€ 24ê°œ ë¯¸ë§Œì´ë©´ ê·¸ ë‚ ì§œë¶€í„° ì¬ìˆ˜ì§‘(í•´ë‹¹ ì¼ì ë°ì´í„° replace)
        # - ì•„ë‹ˆë©´ ë‹¤ìŒ ë‚ ë¶€í„° ìˆ˜ì§‘
        if last_dt:
            last_day = last_dt.date()
            hours = _count_hours_for_day(engine_, gencd, hogi, last_day)
            if hours < 24:
                start_dt = datetime.combine(last_day, datetime.min.time())
            else:
                start_dt = datetime.combine(last_day + timedelta(days=1), datetime.min.time())
        else:
            start_dt = datetime.now() - timedelta(days=365)
        
        if start_dt.date() <= yesterday.date():
            active_targets.append({
                "gencd": gencd,
                "hogi": hogi,
                "plant_name": plant_name,
                "start_dt": start_dt,
            })
            
    print(f"âœ… ì´ {len(active_targets)}ê°œ ë°œì „ì†Œê°€ í™œì„± ìƒíƒœì´ë©° ìˆ˜ì§‘ ëŒ€ìƒì…ë‹ˆë‹¤.")
    return active_targets

# --- [Task 2: API ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬] ---
async def fetch_api_data(session, date_str, gencd, hogi):
    """API í˜¸ì¶œ (strHoki íŒŒë¼ë¯¸í„° ì‚¬ìš©)"""
    params = {
        "serviceKey": _get_api_key(), "pageNo": "1", "numOfRows": "100",
        "strSdate": date_str, "strEdate": date_str,
        "strOrgCd": gencd, "strHoki": str(hogi)
    }
    try:
        async with session.get(ENDPOINT, params=params, timeout=15) as response:
            if response.status != 200: return None
            root = ET.fromstring(await response.text())
            items = root.find('.//items')
            return {child.tag: child.text for child in items} if items is not None else None
    except Exception:
        return None

def _extract_hour0(col: str) -> int:
    # qhorgen01 -> 0, qhorgen24 -> 23
    m = re.search(r"(\d+)$", col)
    if not m:
        raise ValueError(f"ì‹œê°„ ì»¬ëŸ¼ íŒŒì‹± ì‹¤íŒ¨: {col}")
    hour_index = int(m.group(1))
    return hour_index - 1


async def collect_and_save(engine_, targets):
    """í™œì„± ë°œì „ì†Œë“¤ì— ëŒ€í•´ API ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì „ì²˜ë¦¬í•˜ì—¬ DBì— ì €ì¥"""
    yesterday = datetime.now() - timedelta(days=1)
    total_rows = 0

    async with aiohttp.ClientSession() as session:
        for target in targets:
            date_list = pd.date_range(start=target["start_dt"].date(), end=yesterday.date()).strftime("%Y%m%d").tolist()
            if not date_list: continue

            print(f"ğŸ“¡ {target.get('plant_name') or target['gencd']} ({target['hogi']}í˜¸ê¸°) {len(date_list)}ì¼ë¶„ ìˆ˜ì§‘ ì¤‘...")
            
            raw_data = []
            for d_str in date_list:
                data = await fetch_api_data(session, d_str, target["gencd"], target["hogi"])
                if data:
                    data["gencd"], data["hogi"] = target["gencd"], str(target["hogi"])
                    raw_data.append(data)
                await asyncio.sleep(0.05)

            if raw_data:
                # ì „ì²˜ë¦¬: 24ì‹œê°„ ë°ì´í„°ë¥¼ ì„¸ë¡œë¡œ ë³€í™˜
                df_raw = pd.DataFrame(raw_data)
                v_vars = [c for c in df_raw.columns if c.startswith('qhorgen')]
                df_long = df_raw.melt(id_vars=["ymd", "hogi", "gencd", "ipptnm", "qvodgen", "qvodavg", "qvodmax", "qvodmin"], value_vars=v_vars,
                                      var_name='h_str', value_name='generation')
                
                df_long["hour0"] = df_long["h_str"].apply(_extract_hour0).astype(int)
                df_long["datetime"] = pd.to_datetime(df_long["ymd"]) + pd.to_timedelta(df_long["hour0"], unit="h")
                df_long["generation"] = pd.to_numeric(df_long["generation"], errors="coerce").fillna(0)
                df_long["daily_total"] = pd.to_numeric(df_long["qvodgen"], errors="coerce")
                df_long["daily_avg"] = pd.to_numeric(df_long["qvodavg"], errors="coerce")
                df_long["daily_max"] = pd.to_numeric(df_long["qvodmax"], errors="coerce")
                df_long["daily_min"] = pd.to_numeric(df_long["qvodmin"], errors="coerce")
                df_long["plant_name"] = df_long["ipptnm"]
                # API ì‘ë‹µì— ipptnmì´ ì—†ëŠ” ê²½ìš° plant.json ë§¤í•‘ìœ¼ë¡œ ëŒ€ì²´
                if GENCD_TO_NAME:
                    mask_na = df_long["plant_name"].isna() | (df_long["plant_name"].astype(str) == "None")
                    df_long.loc[mask_na, "plant_name"] = df_long.loc[mask_na, "gencd"].map(GENCD_TO_NAME)
                df_long["hogi"] = pd.to_numeric(df_long["hogi"], errors="coerce").astype("Int64")

                final_df = df_long[
                    [
                        "datetime",
                        "gencd",
                        "plant_name",
                        "hogi",
                        "generation",
                        "daily_total",
                        "daily_avg",
                        "daily_max",
                        "daily_min",
                    ]
                ].dropna(subset=["datetime", "gencd", "hogi"])

                with engine_.begin() as conn:
                    # ê°™ì€ ë‚ ì§œ ë°ì´í„°ê°€ ì´ë¯¸ ìˆìœ¼ë©´ replace(íŠ¹íˆ ë§ˆì§€ë§‰ ë‚ ì§œê°€ ë¯¸ì™„ì„±ì¸ ì¼€ì´ìŠ¤)
                    for day in sorted(final_df["datetime"].dt.date.unique()):
                        day_start = datetime.combine(day, datetime.min.time())
                        day_end = day_start + timedelta(days=1)
                        conn.execute(
                            text(
                                """
                                DELETE FROM nambu_generation
                                WHERE gencd = :gencd
                                  AND hogi = :hogi
                                  AND datetime >= :day_start
                                  AND datetime < :day_end
                                """
                            ),
                            {
                                "gencd": target["gencd"],
                                "hogi": int(target["hogi"]),
                                "day_start": day_start,
                                "day_end": day_end,
                            },
                        )

                    final_df.to_sql("nambu_generation", con=conn, if_exists="append", index=False)
                
                total_rows += len(final_df)
                print(f"   ã„´ âœ… {len(final_df)}í–‰ ì €ì¥ ì™„ë£Œ")

    return total_rows

def solar_automation_flow():
    engine = _get_engine()
    # 1. ìˆ˜ì§‘ ëŒ€ìƒ ë¶„ì„
    targets = get_active_targets(engine)

    # 2. ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥
    if targets:
        asyncio.run(collect_and_save(engine, targets))
    else:
        print("â˜€ï¸ ëª¨ë“  ë°œì „ì†Œê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    solar_automation_flow()
